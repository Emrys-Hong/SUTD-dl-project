{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir('..')\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import apex\n",
    "import csv\n",
    "import dataset_word as data\n",
    "import models\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tqdm.notebook import tqdm\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list(input_str):    \n",
    "    return ast.literal_eval(input_str)\n",
    "\n",
    "reports = {}\n",
    "\n",
    "with open(config.cleaned_reports) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "        else:\n",
    "            uid, problems, findings, impression = row[1:]\n",
    "            reports[str(uid)] = (parse_list(problems), findings, impression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report_splits(reports, seed=1337):\n",
    "    uid_list = list(reports.keys())\n",
    "    train_uids, valtest_uids = train_test_split(uid_list, test_size=0.2, random_state=seed)\n",
    "    valid_uids, test_uids = train_test_split(valtest_uids, test_size=0.5, random_state=seed)\n",
    "    \n",
    "    train_reports = {}\n",
    "    valid_reports = {}\n",
    "    test_reports = {}\n",
    "    splits = [train_uids, valid_uids, test_uids]\n",
    "    output_reports = [train_reports, valid_reports, test_reports]\n",
    "    \n",
    "    for i in range(len(splits)):\n",
    "        for uid in splits[i]:\n",
    "            output_reports[i][str(uid)] = reports[str(uid)]\n",
    "            \n",
    "    return output_reports\n",
    "\n",
    "train_reports, valid_reports, _ = create_report_splits(reports)\n",
    "\n",
    "train_dataset = data.XRayDataset(\n",
    "    reports=train_reports,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(2048),\n",
    "        transforms.CenterCrop((2048,2048)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    "))\n",
    "train_dataloader = torch.utils.data.dataloader.DataLoader(train_dataset,\n",
    "                                                          collate_fn=data.collate_fn,\n",
    "                                                          pin_memory=True,\n",
    "                                                          shuffle=True,\n",
    "                                                          batch_size=config.batch_size,\n",
    "                                                          num_workers=config.batch_size)\n",
    "\n",
    "valid_dataset = data.XRayDataset(\n",
    "    reports=valid_reports,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(2048),\n",
    "        transforms.CenterCrop((2048,2048)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "))\n",
    "valid_dataloader = torch.utils.data.dataloader.DataLoader(valid_dataset,\n",
    "                                                          collate_fn=data.collate_fn,\n",
    "                                                          pin_memory=True,\n",
    "                                                          shuffle=True,\n",
    "                                                          batch_size=config.batch_size,\n",
    "                                                          num_workers=config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 1971 x 300\n",
      "Loading embedding file: ./vectors/glove.6B.300d.txt\n",
      "Pre-trained: 1611 (81.74%)\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "memory_format = torch.channels_last\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "encoder = models.EncoderCNN(config.emb_dim, num_classes).to(config.device, memory_format=memory_format)\n",
    "decoder = models.DecoderRNN_Word(config.emb_dim, config.hidden_dim, train_dataset.tokenizer, config.num_layers).to(config.device, memory_format=memory_format)\n",
    "\n",
    "classes_loss = torch.nn.BCEWithLogitsLoss()\n",
    "outputs_loss = torch.nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.parameters())\n",
    "optimizer = apex.optimizers.FusedAdam(params, lr=config.learning_rate)\n",
    "\n",
    "[encoder, decoder], optimizer = apex.amp.initialize([encoder, decoder], optimizer, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader, batch_size, encoder, decoder, classes_loss, outputs_loss, optimizer, train=True):\n",
    "    total_step = len(dataloader.dataset)//batch_size\n",
    "    if train:\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "    else:\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "    running_c_loss = torch.Tensor([0.0])\n",
    "    running_o_loss = torch.Tensor([0.0])\n",
    "    state_h, state_c = decoder.zero_state(batch_size)\n",
    "    state_h = state_h.to(config.device, non_blocking=True)\n",
    "    state_c = state_c.to(config.device, non_blocking=True)\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for i, (images, class_labels, captions, lengths) in enumerate(progress_bar(dataloader)):\n",
    "            images = images.to(config.device, non_blocking=True).contiguous(memory_format=memory_format)\n",
    "            captions = captions.to(config.device, non_blocking=True)\n",
    "            class_labels = class_labels.to(config.device, non_blocking=True)\n",
    "            targets = torch.nn.utils.rnn.pack_padded_sequence(captions, lengths, batch_first=True, enforce_sorted=False)[0]\n",
    "            encoder.zero_grad()\n",
    "            decoder.zero_grad()\n",
    "            logits, features = encoder(images)\n",
    "            c_loss = classes_loss(logits, class_labels)\n",
    "            outputs, (state_h, state_c) = decoder(features, captions, lengths, (state_h, state_c))\n",
    "            o_loss = outputs_loss(outputs, targets)\n",
    "            if train:\n",
    "                with apex.amp.scale_loss(c_loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward(retain_graph=True)\n",
    "                with apex.amp.scale_loss(o_loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                state_h = state_h.detach()\n",
    "                state_c = state_c.detach()\n",
    "                optimizer.step()\n",
    "            running_c_loss += c_loss\n",
    "            running_o_loss += o_loss\n",
    "    c_loss = float(running_c_loss.item()/total_step)\n",
    "    o_loss = float(running_o_loss.item()/total_step)\n",
    "    return c_loss, o_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "\n",
      "Epoch 1 / 2 :\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='544' class='' max='544', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [544/544 03:02<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='66' class='' max='66', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [66/66 00:13<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss -  0.153 4.378 - perplexity - 79.66 - valid_loss -  0.139 6.912 - perplexity - 1004.264\n",
      "\n",
      "Epoch 2 / 2 :\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='544' class='' max='544', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [544/544 02:59<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='66' class='' max='66', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [66/66 00:13<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss -  0.147 3.273 - perplexity - 26.387 - valid_loss -  0.143 7.494 - perplexity - 1798.074\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "print(\"Start training\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"\\nEpoch\", epoch+1, \"/\", num_epochs, \":\\n\")\n",
    "    \n",
    "    train_c_loss, train_o_loss = train_one_epoch(train_dataloader, config.batch_size, encoder, decoder, classes_loss, outputs_loss, optimizer, train=True)\n",
    "    valid_c_loss, valid_o_loss = train_one_epoch(valid_dataloader, config.batch_size, encoder, decoder, classes_loss, outputs_loss, optimizer, train=False)\n",
    "\n",
    "    print(\"train_loss - \", round(train_c_loss,3),round(train_o_loss,3), \"- perplexity -\", round(np.exp(train_o_loss),3), \n",
    "          \"- valid_loss - \", round(valid_c_loss,3),round(valid_o_loss,3), \"- perplexity -\", round(np.exp(valid_o_loss),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(encoder.state_dict(), 'save/encoder_word.pt')\n",
    "torch.save(decoder.state_dict(), 'save/decoder_word.pt')\n",
    "encoder.load_state_dict(torch.load('save/encoder_word.pt'))\n",
    "decoder.load_state_dict(torch.load('save/decoder_word.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN_Word(\n",
       "  (embed): Embeddings(\n",
       "    (lut): Embedding(1971, 300, padding_idx=0)\n",
       "  )\n",
       "  (lstm): LSTM(300, 128, num_layers=3, batch_first=True)\n",
       "  (linear): Linear(in_features=128, out_features=1971, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (norm): LayerNorm((300,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-33e57fdd006b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for index in range(0, 10):\n",
    "    image, _, impression = train_dataset.__getitem__(index)\n",
    "    image_tensor = image.unsqueeze(0).to(config.device)\n",
    "    logits, feature = encoder(image_tensor)\n",
    "    sampled_ids = decoder.sample(feature)\n",
    "    sampled_ids = list(sampled_ids[0].cpu().numpy())\n",
    "\n",
    "    plt.title(\"Image: \"+str(index))\n",
    "    plt_img = np.moveaxis(image.numpy(), 0, -1)\n",
    "    plt.imshow(plt_img)\n",
    "    plt.show()\n",
    "\n",
    "    print(\" Original:\", train_dataset.tokenizer.decode(impression))\n",
    "    print(\"Generated:\", train_dataset.tokenizer.decode(sampled_ids))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
