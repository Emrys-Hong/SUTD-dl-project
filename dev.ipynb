{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import apex\n",
    "import csv\n",
    "import data\n",
    "import models\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "* Image size: torch.Size([3, 2048, 2048])\n",
      "* Impression: tensor([39, 40,  0, 26, 28, 46, 45, 30,  0, 28, 33, 26, 39, 32, 30, 44,  0, 31,\n",
      "        43, 40, 38,  0, 41, 43, 34, 40, 43,  0, 34, 38, 26, 32, 34, 39, 32,  8])\n",
      "* Vocab:\n",
      "['4', 'g', '9', 'n', 'b', '3', '6', '1', 's', 'p', 'x', 'y', 'z', ';', ',', '/', \"'\", 'j', '&', 'e', '>', 'k', 'r', 'q', '<', 'v', ')', 'd', '2', 'm', '-', 'f', 'a', 'l', 'h', '.', 'o', ' ', 'w', 'i', '5', '%', '[', '7', 'u', ']', 'c', 't', '8', '0', '(', ':']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "def parse_list(input_str):    \n",
    "    return ast.literal_eval(input_str)\n",
    "\n",
    "reports = {}\n",
    "\n",
    "with open(\"./cleaned_reports.csv\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "        else:\n",
    "            uid, problems, findings, impression = row[1:]\n",
    "            reports[str(uid)] = (parse_list(problems), findings, impression)\n",
    "\n",
    "def create_report_splits(reports, seed=1337):\n",
    "    uid_list = list(reports.keys())\n",
    "    train_uids, valtest_uids = train_test_split(uid_list, test_size=0.2, random_state=seed)\n",
    "    valid_uids, test_uids = train_test_split(valtest_uids, test_size=0.5, random_state=seed)\n",
    "    \n",
    "    train_reports = {}\n",
    "    valid_reports = {}\n",
    "    test_reports = {}\n",
    "    splits = [train_uids, valid_uids, test_uids]\n",
    "    output_reports = [train_reports, valid_reports, test_reports]\n",
    "    \n",
    "    for i in range(len(splits)):\n",
    "        for uid in splits[i]:\n",
    "            output_reports[i][str(uid)] = reports[str(uid)]\n",
    "            \n",
    "    return output_reports\n",
    "\n",
    "train_reports, _, _ = create_report_splits(reports)\n",
    "\n",
    "train_dataset = data.XRayDataset(\n",
    "    reports=train_reports,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(2048),\n",
    "        transforms.CenterCrop((2048,2048)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "))\n",
    "\n",
    "print(\"Sample:\")\n",
    "image, impression = train_dataset.__getitem__(0)\n",
    "print(\"* Image size:\", image.size())\n",
    "print(\"* Impression:\", impression)\n",
    "print(\"* Vocab:\")\n",
    "print(train_dataset.vocab)\n",
    "\n",
    "train_dataloader = torch.utils.data.dataloader.DataLoader(train_dataset,\n",
    "                                                          collate_fn=data.collate_fn,\n",
    "                                                          pin_memory=True,\n",
    "                                                          shuffle=True,\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          num_workers=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "learning_rate = 0.001\n",
    "memory_format = torch.channels_last\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = models.EncoderCNN(embed_size).to(device, memory_format=memory_format)\n",
    "decoder = models.DecoderRNN(embed_size, hidden_size, len(train_dataset.vocab), num_layers).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.parameters())\n",
    "optimizer = apex.optimizers.FusedAdam(params, lr=learning_rate)\n",
    "\n",
    "[encoder, decoder], optimizer = apex.amp.initialize([encoder, decoder], optimizer, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "\n",
      "Epoch 1 / 4 :\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25eab6616d644d2c9db7a04e18047a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=339.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "total_step = len(train_dataloader.dataset)//batch_size\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "print(\"Start training\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"\\nEpoch\", epoch+1, \"/\", num_epochs, \":\\n\")\n",
    "    for i, (images, captions, lengths) in enumerate(tqdm(train_dataloader, total=total_step)):\n",
    "\n",
    "        # Set mini-batch dataset\n",
    "        images = images.cuda(non_blocking=True).contiguous(memory_format=memory_format)\n",
    "        captions = captions.cuda(non_blocking=True).contiguous()\n",
    "        targets = torch.nn.utils.rnn.pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "        \n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        with apex.amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"train_loss - \", loss.item(), \"- perplexity -\", np.exp(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(0, 10):\n",
    "    image, impression = train_dataset.__getitem__(index)\n",
    "    image_tensor = image.unsqueeze(0).cuda()\n",
    "    feature = encoder(image_tensor)\n",
    "    sampled_ids = decoder.sample(feature)\n",
    "    sampled_ids = list(sampled_ids[0].cpu().numpy())\n",
    "\n",
    "    plt.title(\"Image: \"+str(index))\n",
    "    plt_img = np.moveaxis(image.numpy(), 0, -1)\n",
    "    plt.imshow(plt_img)\n",
    "    plt.show()\n",
    "\n",
    "    print(\" Original:\", train_dataset.tokenizer.decode(impression))\n",
    "    print(\"Generated:\", train_dataset.tokenizer.decode(sampled_ids))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
