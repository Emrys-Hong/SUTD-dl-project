{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import apex\n",
    "import data\n",
    "import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reports: 3851\n",
      "Skipped: 3648 images\n",
      "Sample:\n",
      "* Image size: torch.Size([3, 2048, 2048])\n",
      "* Impression: tensor([37, 38, 41, 36, 24, 35,  0, 26, 31, 28, 42, 43,  0, 47,  6, 47, 47, 47,\n",
      "        47,  7])\n",
      "* Vocab:\n",
      "['k', 'o', 'i', 'p', 'j', \"'\", ')', 'd', 'x', '%', ':', '4', 'f', '5', 'e', '[', 'l', '1', 'c', 's', 'u', 'b', '9', '3', '0', '\"', 'z', 'q', ' ', 'm', 'h', 'y', '6', 't', '-', 'r', 'n', 'w', '/', '>', '8', '7', 'g', '2', '.', 'a', 'v', '<', '(', ';']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataset = data.XRayDataset(\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(2048),\n",
    "        transforms.CenterCrop((2048,2048)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "))\n",
    "\n",
    "print(\"Sample:\")\n",
    "image, impression = train_dataset.__getitem__(0)\n",
    "print(\"* Image size:\", image.size())\n",
    "print(\"* Impression:\", impression)\n",
    "print(\"* Vocab:\")\n",
    "print(train_dataset.vocab)\n",
    "\n",
    "train_dataloader = torch.utils.data.dataloader.DataLoader(train_dataset,\n",
    "                                                          collate_fn=data.collate_fn,\n",
    "                                                          pin_memory=True,\n",
    "                                                          shuffle=True,\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          num_workers=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "learning_rate = 0.001\n",
    "memory_format = torch.channels_last\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = models.EncoderCNN(embed_size).to(device, memory_format=memory_format)\n",
    "decoder = models.DecoderRNN(embed_size, hidden_size, len(train_dataset.vocab), num_layers).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.parameters())\n",
    "optimizer = apex.optimizers.FusedAdam(params, lr=learning_rate)\n",
    "\n",
    "[encoder, decoder], optimizer = apex.amp.initialize([encoder, decoder], optimizer, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch [0/4], Step [0/435], Loss: 3.9344, Perplexity: 51.1323\n",
      "Epoch [0/4], Step [100/435], Loss: 2.9513, Perplexity: 19.1316\n",
      "Epoch [0/4], Step [200/435], Loss: 2.5882, Perplexity: 13.3057\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "total_step = len(train_dataloader.dataset)//batch_size\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "print(\"Start training\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, captions, lengths) in enumerate(train_dataloader):\n",
    "\n",
    "        # Set mini-batch dataset\n",
    "        images = images.cuda(non_blocking=True).contiguous(memory_format=memory_format)\n",
    "        captions = captions.cuda(non_blocking=True).contiguous()\n",
    "        targets = torch.nn.utils.rnn.pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "        \n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        with apex.amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print log info\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}\"\n",
    "                  .format(epoch, num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(0, 10):\n",
    "    image, impression = train_dataset.__getitem__(index)\n",
    "    image_tensor = image.unsqueeze(0).cuda()\n",
    "    feature = encoder(image_tensor)\n",
    "    sampled_ids = decoder.sample(feature)\n",
    "    sampled_ids = list(sampled_ids[0].cpu().numpy())\n",
    "\n",
    "    plt.title(\"Image: \"+str(index))\n",
    "    plt_img = np.moveaxis(image.numpy(), 0, -1)\n",
    "    plt.imshow(plt_img)\n",
    "    plt.show()\n",
    "\n",
    "    print(\" Original:\", train_dataset.tokenizer.decode(impression))\n",
    "    print(\"Generated:\", train_dataset.tokenizer.decode(sampled_ids))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
